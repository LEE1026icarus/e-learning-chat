# import ollama
# import streamlit as st

# st.title("Ollama Python Chatbot")

# # initialize history
# if "messages" not in st.session_state:
#     st.session_state["messages"] = []

# # init models
# if "model" not in st.session_state:
#     st.session_state["model"] = ""

# models = [model["name"] for model in ollama.list()["models"]]
# st.session_state["model"] = st.selectbox("Choose your model", models)

# def model_res_generator():
#     stream = ollama.chat(
#         model=st.session_state["model"],
#         messages=st.session_state["messages"],
#         stream=True,
#     )
#     for chunk in stream:
#         yield chunk["message"]["content"]

# # Display chat messages from history on app rerun
# for message in st.session_state["messages"]:
#     with st.chat_message(message["role"]):
#         st.markdown(message["content"])

# if prompt := st.chat_input("What is up?"):
#     # add latest message to history in format {role, content}
#     st.session_state["messages"].append({"role": "user", "content": prompt})

#     with st.chat_message("user"):
#         st.markdown(prompt)

#     with st.chat_message("assistant"):
#         message = st.write_stream(model_res_generator())
#         st.session_state["messages"].append({"role": "assistant", "content": message})

# import streamlit as st
# from langchain_community.vectorstores import Chroma
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain.prompts import PromptTemplate
# from langchain_community.chat_models import ChatOllama
# from langchain.chains.question_answering import load_qa_chain
# from dotenv import load_dotenv
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.runnables import RunnablePassthrough


# st.header('í•œê¸°ëŒ€ AI íœ´ë¨¼ ê°•ì˜ í”„ë¡œì íŠ¸', divider='rainbow')
# st.markdown('''ê¶ê¸ˆí•œê²Œ ìˆìœ¼ì‹œë©´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!! :balloon:''')

# # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# load_dotenv()

# # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ ëª¨ë¸ ì´ˆê¸°í™”
# persist_directory = 'db'
# embedding = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask')
# vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)

# class ChatGuide:
#     def __init__(self, model_name='llama3'):
#         self.model = ChatOllama(model=model_name)
#         self.prompt = PromptTemplate.from_template(
#             """
#             ë‹¹ì‹ ì€ í•™êµ ì˜¨ë¼ì¸ ê°•ì˜ í”Œë«í¼ì—ì„œ í•™ìƒë“¤ì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI íŠœí„°ì…ë‹ˆë‹¤.
#             <ì§€ì¹¨>
#             1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìµœëŒ€í•œ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
#             2. ë‹µë³€ì€ í•™ìƒì˜ ìˆ˜ì¤€ì— ë§ëŠ” ì‰¬ìš´ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ë˜, ì „ë¬¸ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
#             3. ë‹µë³€ì— ê´€ë ¨ëœ í•µì‹¬ ê°œë…ì´ë‚˜ ìš©ì–´ê°€ ìˆë‹¤ë©´ ê°„ë‹¨íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
#             4. ì‹¤ìƒí™œ ì˜ˆì‹œë‚˜ ì‹œê° ìë£Œ(ì´ë¯¸ì§€, ê·¸ë˜í”„, ë‹¤ì´ì–´ê·¸ë¨ ë“±)ë¥¼ í™œìš©í•˜ì—¬ ì´í•´ë¥¼ ë•ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì˜ˆì‹œëŠ” <ì˜ˆì‹œ></ì˜ˆì‹œ> íƒœê·¸ë¡œ ê°ì‹¸ ì£¼ì„¸ìš”.
#             5. ì¶”ê°€ í•™ìŠµì— ë„ì›€ì´ ë  ë§Œí•œ ìë£Œë‚˜ ì°¸ê³  ë¬¸í—Œì´ ìˆë‹¤ë©´ ë§í¬ ë˜ëŠ” ì¶œì²˜ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.
#             6. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë‹µë³€í•˜ê¸° ì–´ë ¤ìš´ ê²½ìš°, ê´€ë ¨ ì •ë³´ê°€ ë¶€ì¡±í•¨ì„ ì•Œë¦¬ê³  ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìš”ì²­í•´ ì£¼ì„¸ìš”.
#             7. ë°˜ë“œì‹œ ë‹µë³€ì€ í•œêµ­ì–´ë¡œë§Œ í•´ì£¼ì„¸ìš”.
#             8. ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
#             </ì§€ì¹¨>
#             <ì»¨í…ìŠ¤íŠ¸>
#             {context}
#             </ì»¨í…ìŠ¤íŠ¸>
#             <í•™ìƒ ì§ˆë¬¸>
#             {question}
#             </í•™ìƒ ì§ˆë¬¸>
#             <ë‹µë³€>
#             """
#         )
        
#         # ê¸°ì¡´ì˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ retriever ì„¤ì •
#         self.retriever = vectordb.as_retriever(search_kwargs={"k": 2})

#         # QA ì²´ì¸ì„ êµ¬ì„±
#         self.chain = ({"context": self.retriever, "question": RunnablePassthrough()}
#             | self.prompt
#             | self.model  
#             | StrOutputParser())
        
#     def ask(self, query: str):
#         return self.chain.invoke(query)

# st.title("ê²½ì˜ì •ë³´ì‹œìŠ¤í…œê°œë¡  Chatbot")
# import time

# if st.button('Three cheers'):
#     st.toast('MISê°€ ë¬´ì—‡ì…ë‹ˆê¹Œ?!')
#     time.sleep(.5)
#     st.toast('MISê°€ ìµœê·¼ ëŒ€ë‘ë˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?')
#     time.sleep(.5)
#     st.toast('MISë¥¼ ë„ì…í•œ ê¸°ì—…ë“¤ì˜ ì‚¬ë¡€ë¥¼ ì•Œë ¤ì¤˜', icon='ğŸ‰')
             
# # initialize history 
# if "messages" not in st.session_state:
#     st.session_state["messages"] = []

# # init ChatGuide
# if "chat_guide" not in st.session_state:
#     st.session_state["chat_guide"] = ChatGuide(model_name="llama3")

# def model_res_generator(query):
#     response = st.session_state["chat_guide"].ask(query)
#     yield response

# # Display chat messages from history on app rerun
# for message in st.session_state["messages"]:
#     with st.chat_message(message["role"]):
#         st.markdown(message["content"])

# if prompt := st.chat_input("What is up?"):
#     # add latest message to history in format {role, content}
#     st.session_state["messages"].append({"role": "user", "content": prompt}) 
    
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     with st.chat_message("assistant"):
#         message = st.write_stream(model_res_generator(prompt))
        
#     st.session_state["messages"].append({"role": "assistant", "content": message})

import streamlit as st
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatOllama
from langchain.chains.question_answering import load_qa_chain
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from googletrans import Translator

st.header('í•œê¸°ëŒ€ AI íœ´ë¨¼ ê°•ì˜ í”„ë¡œì íŠ¸', divider='rainbow')
st.markdown('''ê¶ê¸ˆí•œê²Œ ìˆìœ¼ì‹œë©´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!! :balloon:''')

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ ëª¨ë¸ ì´ˆê¸°í™”
persist_directory = 'db'
embedding = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask')
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)

class ChatGuide:
    def __init__(self, model_name='llama3'):
        self.model = ChatOllama(model=model_name)
        self.prompt = PromptTemplate.from_template(
            """
            ë‹¹ì‹ ì€ í•™êµ ì˜¨ë¼ì¸ ê°•ì˜ í”Œë«í¼ì—ì„œ í•™ìƒë“¤ì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI íŠœí„°ì…ë‹ˆë‹¤.
            <ì§€ì¹¨>
            1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìµœëŒ€í•œ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
            2. ë‹µë³€ì€ í•™ìƒì˜ ìˆ˜ì¤€ì— ë§ëŠ” ì‰¬ìš´ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ë˜, ì „ë¬¸ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
            3. ë‹µë³€ì— ê´€ë ¨ëœ í•µì‹¬ ê°œë…ì´ë‚˜ ìš©ì–´ê°€ ìˆë‹¤ë©´ ê°„ë‹¨íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
            4. ì‹¤ìƒí™œ ì˜ˆì‹œë‚˜ ì‹œê° ìë£Œ(ì´ë¯¸ì§€, ê·¸ë˜í”„, ë‹¤ì´ì–´ê·¸ë¨ ë“±)ë¥¼ í™œìš©í•˜ì—¬ ì´í•´ë¥¼ ë•ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì˜ˆì‹œëŠ” <ì˜ˆì‹œ></ì˜ˆì‹œ> íƒœê·¸ë¡œ ê°ì‹¸ ì£¼ì„¸ìš”.
            5. ì¶”ê°€ í•™ìŠµì— ë„ì›€ì´ ë  ë§Œí•œ ìë£Œë‚˜ ì°¸ê³  ë¬¸í—Œì´ ìˆë‹¤ë©´ ë§í¬ ë˜ëŠ” ì¶œì²˜ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.
            6. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë‹µë³€í•˜ê¸° ì–´ë ¤ìš´ ê²½ìš°, ê´€ë ¨ ì •ë³´ê°€ ë¶€ì¡±í•¨ì„ ì•Œë¦¬ê³  ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìš”ì²­í•´ ì£¼ì„¸ìš”.
            7. ë°˜ë“œì‹œ ë‹µë³€ì€ í•œêµ­ì–´ë¡œë§Œ í•´ì£¼ì„¸ìš”.
            8. ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
            </ì§€ì¹¨>
            <ì»¨í…ìŠ¤íŠ¸>
            {context}
            </ì»¨í…ìŠ¤íŠ¸>
            <í•™ìƒ ì§ˆë¬¸>
            {question}
            </í•™ìƒ ì§ˆë¬¸>
            <ë‹µë³€>
            """
        )
        
        # ê¸°ì¡´ì˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ retriever ì„¤ì •
        self.retriever = vectordb.as_retriever(search_kwargs={"k": 2})

        # QA ì²´ì¸ì„ êµ¬ì„±
        self.chain = ({"context": self.retriever, "question": RunnablePassthrough()}
            | self.prompt
            | self.model  
            | StrOutputParser())
        
    def ask(self, query: str):
        return self.chain.invoke(query)

# ChatGuide ì´ˆê¸°í™”
if "chat_guide" not in st.session_state:
    st.session_state["chat_guide"] = ChatGuide(model_name="llama3")

# ë²ˆì—­ê¸° ì´ˆê¸°í™”
translator = Translator()

# Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì„±
st.title("ê²½ì˜ì •ë³´ì‹œìŠ¤í…œê°œë¡  Chatbot")

with st.chat_message("user"):
    st.write("MISê°€ ë¬´ì—‡ì…ë‹ˆê¹Œ?!")
    st.write("MISê°€ ìµœê·¼ ëŒ€ë‘ë˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?")
    st.write("MISë¥¼ ë„ì…í•œ ê¸°ì—…ë“¤ì˜ ì‚¬ë¡€ë¥¼ ì•Œë ¤ì¤˜")
                 
# ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

def model_res_generator(query):
    response = st.session_state["chat_guide"].ask(query)
    return response

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state["messages"]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("What is up?"):
    # ì‚¬ìš©ì ì…ë ¥ ê¸°ë¡
    st.session_state["messages"].append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ëª¨ë¸ ì‘ë‹µ ìƒì„±
    response = model_res_generator(prompt)
    
    # ëª¨ë¸ ì‘ë‹µì„ í•œêµ­ì–´ë¡œ ë²ˆì—­
    translated_response = translator.translate(response, src='en', dest='ko').text
    
    with st.chat_message("assistant"):
        st.markdown(translated_response)
        
    st.session_state["messages"].append({"role": "assistant", "content": translated_response})
